
Download and Clean Dataset
Let's start by importing the pandas and the Numpy libraries.

In [1]:
import pandas as pd
import numpy as np
We will be using the dataset provided in the assignment

The dataset is about the compressive strength of different samples of concrete based on the volumes of the different ingredients that were used to make them. Ingredients include:

1. Cement

2. Blast Furnace Slag

3. Fly Ash

4. Water

5. Superplasticizer

6. Coarse Aggregate

7. Fine Aggregate

Let's read the dataset into a pandas dataframe.

In [2]:
concrete_data = pd.read_csv('concrete_data.csv')
concrete_data.head()
Out[2]:
Cement	Blast Furnace Slag	Fly Ash	Water	Superplasticizer	Coarse Aggregate	Fine Aggregate	Age	Strength
0	540.0	0.0	0.0	162.0	2.5	1040.0	676.0	28	79.99
1	540.0	0.0	0.0	162.0	2.5	1055.0	676.0	28	61.89
2	332.5	142.5	0.0	228.0	0.0	932.0	594.0	270	40.27
3	332.5	142.5	0.0	228.0	0.0	932.0	594.0	365	41.05
4	198.6	132.4	0.0	192.0	0.0	978.4	825.5	360	44.30
So the first concrete sample has 540 cubic meter of cement, 0 cubic meter of blast furnace slag, 0 cubic meter of fly ash, 162 cubic meter of water, 2.5 cubic meter of superplaticizer, 1040 cubic meter of coarse aggregate, 676 cubic meter of fine aggregate. Such a concrete mix which is 28 days old, has a compressive strength of 79.99 MPa.

Let's check how many data points we have.
In [3]:
concrete_data.shape
Out[3]:
(1030, 9)
So, there are approximately 1000 samples to train our model on. Because of the few samples, we have to be careful not to overfit the training data.

Let's check the dataset for any missing values.

In [4]:
concrete_data.describe()
Out[4]:
Cement	Blast Furnace Slag	Fly Ash	Water	Superplasticizer	Coarse Aggregate	Fine Aggregate	Age	Strength
count	1030.000000	1030.000000	1030.000000	1030.000000	1030.000000	1030.000000	1030.000000	1030.000000	1030.000000
mean	281.167864	73.895825	54.188350	181.567282	6.204660	972.918932	773.580485	45.662136	35.817961
std	104.506364	86.279342	63.997004	21.354219	5.973841	77.753954	80.175980	63.169912	16.705742
min	102.000000	0.000000	0.000000	121.800000	0.000000	801.000000	594.000000	1.000000	2.330000
25%	192.375000	0.000000	0.000000	164.900000	0.000000	932.000000	730.950000	7.000000	23.710000
50%	272.900000	22.000000	0.000000	185.000000	6.400000	968.000000	779.500000	28.000000	34.445000
75%	350.000000	142.950000	118.300000	192.000000	10.200000	1029.400000	824.000000	56.000000	46.135000
max	540.000000	359.400000	200.100000	247.000000	32.200000	1145.000000	992.600000	365.000000	82.600000
In [5]:
concrete_data.isnull().sum()
Out[5]:
Cement                0
Blast Furnace Slag    0
Fly Ash               0
Water                 0
Superplasticizer      0
Coarse Aggregate      0
Fine Aggregate        0
Age                   0
Strength              0
dtype: int64
The data looks very clean and is ready to be used to build our model.

Split data into predictors and target
The target variable in this problem is the concrete sample strength. Therefore, our predictors will be all the other columns.

In [6]:
concrete_data_columns = concrete_data.columns
predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength
target = concrete_data['Strength'] # Strength column
Let's do a quick sanity check of the predictors and the target dataframes.

In [7]:
predictors.head()
Out[7]:
Cement	Blast Furnace Slag	Fly Ash	Water	Superplasticizer	Coarse Aggregate	Fine Aggregate	Age
0	540.0	0.0	0.0	162.0	2.5	1040.0	676.0	28
1	540.0	0.0	0.0	162.0	2.5	1055.0	676.0	28
2	332.5	142.5	0.0	228.0	0.0	932.0	594.0	270
3	332.5	142.5	0.0	228.0	0.0	932.0	594.0	365
4	198.6	132.4	0.0	192.0	0.0	978.4	825.5	360
In [8]:
target.head()
Out[8]:
0    79.99
1    61.89
2    40.27
3    41.05
4    44.30
Name: Strength, dtype: float64
Finally, the last step is to normalize the data by substracting the mean and dividing by the standard deviation.

In [9]:
predictors_norm = (predictors - predictors.mean()) / predictors.std()
predictors_norm.head()
Out[9]:
Cement	Blast Furnace Slag	Fly Ash	Water	Superplasticizer	Coarse Aggregate	Fine Aggregate	Age
0	2.476712	-0.856472	-0.846733	-0.916319	-0.620147	0.862735	-1.217079	-0.279597
1	2.476712	-0.856472	-0.846733	-0.916319	-0.620147	1.055651	-1.217079	-0.279597
2	0.491187	0.795140	-0.846733	2.174405	-1.038638	-0.526262	-2.239829	3.551340
3	0.491187	0.795140	-0.846733	2.174405	-1.038638	-0.526262	-2.239829	5.055221
4	-0.790075	0.678079	-0.846733	0.488555	-1.038638	0.070492	0.647569	4.976069
In [10]:
n_cols = predictors_norm.shape[1] # number of predictors
n_cols
Out[10]:
8


Import Keras
Let's go ahead and import the Keras library
In [11]:
import keras
Using TensorFlow backend.
As you can see, the TensorFlow backend was used to install the Keras library.

Let's import the rest of the packages from the Keras library that we will need to build our regressoin model.

In [12]:
from keras.models import Sequential
from keras.layers import Dense
In [13]:
# define regression model
def regression_model():
    # create model
    model = Sequential()
    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))
    model.add(Dense(1))
    
    # compile model
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model
The above function creates a model that has one hidden layer with 10 neurons and a ReLU activation function. It uses the adam optimizer and the mean squared error as the loss function.

Let's import scikit-learn in order to randomly split the data into a training and test sets

In [14]:
from sklearn.model_selection import train_test_split
Splitting the data into a training and test sets by holding 30% of the data for testing

In [15]:
X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=42)
Train and Test the Network
Let's call the function now to create our model.

In [16]:
# build the model
model = regression_model()
Next, we will train the model for 50 epochs.

In [17]:
# fit the model
epochs = 50
model.fit(X_train, y_train, epochs=epochs, verbose=2)
Epoch 1/50
 - 0s - loss: 1651.1822
Epoch 2/50
 - 0s - loss: 1631.2427
Epoch 3/50
 - 0s - loss: 1611.8665
Epoch 4/50
 - 0s - loss: 1593.0129
Epoch 5/50
 - 0s - loss: 1574.5028
Epoch 6/50
 - 0s - loss: 1556.1718
Epoch 7/50
 - 0s - loss: 1538.1068
Epoch 8/50
 - 0s - loss: 1520.4636
Epoch 9/50
 - 0s - loss: 1502.4612
Epoch 10/50
 - 0s - loss: 1484.6265
Epoch 11/50
 - 0s - loss: 1466.5279
Epoch 12/50
 - 0s - loss: 1448.3623
Epoch 13/50
 - 0s - loss: 1429.3800
Epoch 14/50
 - 0s - loss: 1410.6920
Epoch 15/50
 - 0s - loss: 1391.1524
Epoch 16/50
 - 0s - loss: 1371.3942
Epoch 17/50
 - 0s - loss: 1351.2343
Epoch 18/50
 - 0s - loss: 1330.5209
Epoch 19/50
 - 0s - loss: 1309.4222
Epoch 20/50
 - 0s - loss: 1287.7469
Epoch 21/50
 - 0s - loss: 1265.5437
Epoch 22/50
 - 0s - loss: 1242.3534
Epoch 23/50
 - 0s - loss: 1219.0512
Epoch 24/50
 - 0s - loss: 1194.1156
Epoch 25/50
 - 0s - loss: 1169.3435
Epoch 26/50
 - 0s - loss: 1143.0799
Epoch 27/50
 - 0s - loss: 1116.2312
Epoch 28/50
 - 0s - loss: 1088.7420
Epoch 29/50
 - 0s - loss: 1060.6200
Epoch 30/50
 - 0s - loss: 1032.4794
Epoch 31/50
 - 0s - loss: 1003.6558
Epoch 32/50
 - 0s - loss: 974.9272
Epoch 33/50
 - 0s - loss: 946.0666
Epoch 34/50
 - 0s - loss: 917.1510
Epoch 35/50
 - 0s - loss: 888.5678
Epoch 36/50
 - 0s - loss: 859.9041
Epoch 37/50
 - 0s - loss: 831.3088
Epoch 38/50
 - 0s - loss: 803.0652
Epoch 39/50
 - 0s - loss: 775.1644
Epoch 40/50
 - 0s - loss: 747.1952
Epoch 41/50
 - 0s - loss: 720.1013
Epoch 42/50
 - 0s - loss: 693.5036
Epoch 43/50
 - 0s - loss: 667.2098
Epoch 44/50
 - 0s - loss: 641.2060
Epoch 45/50
 - 0s - loss: 616.5792
Epoch 46/50
 - 0s - loss: 592.0672
Epoch 47/50
 - 0s - loss: 568.0495
Epoch 48/50
 - 0s - loss: 545.4204
Epoch 49/50
 - 0s - loss: 522.8632
Epoch 50/50
 - 0s - loss: 501.0779
Out[17]:
<keras.callbacks.callbacks.History at 0x7fbde8453e10>
Next we need to evaluate the model on the test data.

In [18]:
loss_val = model.evaluate(X_test, y_test)
y_pred = model.predict(X_test)
loss_val
309/309 [==============================] - 0s 73us/step
Out[18]:
493.70862813906376
Now we need to compute the mean squared error between the predicted concrete strength and the actual concrete strength.

Let's import the mean_squared_error function from Scikit-learn.

In [19]:
from sklearn.metrics import mean_squared_error
In [20]:
mean_square_error = mean_squared_error(y_test, y_pred)
mean = np.mean(mean_square_error)
standard_deviation = np.std(mean_square_error)
print(mean, standard_deviation)
493.70863791330663 0.0
Create a list of 50 mean squared errors and report mean and the standard deviation of the mean squared errors.

In [22]:
total_mean_squared_errors = 50
epochs = 50
mean_squared_errors = []
for i in range(0, total_mean_squared_errors):
    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)
    model.fit(X_train, y_train, epochs=epochs, verbose=0)
    MSE = model.evaluate(X_test, y_test, verbose=0)
    print("MSE "+str(i+1)+": "+str(MSE))
    y_pred = model.predict(X_test)
    mean_square_error = mean_squared_error(y_test, y_pred)
    mean_squared_errors.append(mean_square_error)

mean_squared_errors = np.array(mean_squared_errors)
mean = np.mean(mean_squared_errors)
standard_deviation = np.std(mean_squared_errors)

print('\n')
print("Below is the mean and standard deviation of " +str(total_mean_squared_errors) + " mean squared errors with normalized data. Total number of epochs for each training is: " +str(epochs) + "\n")
print("Mean: "+str(mean))
print("Standard Deviation: "+str(standard_deviation))
MSE 1: 96.23834398880746
MSE 2: 82.52934939190023
MSE 3: 50.423970811961155
MSE 4: 48.99604903532849
MSE 5: 44.70407389668585
MSE 6: 45.84412700072847
MSE 7: 47.875012147773816
MSE 8: 35.477522495109284
MSE 9: 35.563788170181816
MSE 10: 36.80308609255695
MSE 11: 35.19219320105889
MSE 12: 34.34737917211835
MSE 13: 40.38878035313875
MSE 14: 38.32519055956004
MSE 15: 32.740622634640786
MSE 16: 27.634737857337136
MSE 17: 31.34054788879592
MSE 18: 32.40728301755047
MSE 19: 31.37120697490606
MSE 20: 31.427853618239123
MSE 21: 29.94852909223933
MSE 22: 30.7034236080824
MSE 23: 25.380024011852672
MSE 24: 28.135550983515373
MSE 25: 32.137249283034436
MSE 26: 32.515969162234214
MSE 27: 26.966679705771043
MSE 28: 28.428698246533045
MSE 29: 31.233512199426546
MSE 30: 29.140160242716473
MSE 31: 27.48099442515944
MSE 32: 27.894654844956875
MSE 33: 25.82127880355687
MSE 34: 28.76602467904199
MSE 35: 32.21809555103092
MSE 36: 32.908837167190505
MSE 37: 24.138178942658755
MSE 38: 30.938592262638426
MSE 39: 29.289363231473757
MSE 40: 24.388125768371385
MSE 41: 30.36188562633922
MSE 42: 25.28865729643689
MSE 43: 27.72400673924912
MSE 44: 33.43268985192753
MSE 45: 30.220784554589528
MSE 46: 30.706770295078314
MSE 47: 28.18811243791796
MSE 48: 30.8071376961026
MSE 49: 31.410146176236346
MSE 50: 30.94078703451311


Below is the mean and standard deviation of 50 mean squared errors with normalized data. Total number of epochs for each training is: 50

Mean: 34.742920380277766
Standard Deviation: 12.76633575833323
In [ ]:
